#https://github.com/prometheus-community/helm-charts/tree/prometheus-15.10.5/charts/prometheus
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: app-set-prometheus
  namespace: argocd
spec:
  generators:
    - list:
        elements:
          - cluster: in-cluster
            url: https://kubernetes.default.svc
  template:
    metadata:
      name: '{{cluster}}-prometheus'
    spec:
      project: infra-project
      revisionHistoryLimit: 1
      source:
        chart: prometheus
        repoURL: https://prometheus-community.github.io/helm-charts/
        targetRevision: 15.10.5
        # targetRevision: 25.1.0
        helm:
          values: |

            rbac:
              create: true
            podSecurityPolicy:
              enabled: false
            imagePullSecrets:
            alertmanager:
              enabled: true
              image:
                repository: quay.io/prometheus/alertmanager
                tag: v0.24.0
                pullPolicy: IfNotPresent
              baseURL: "http://localhost:9093"
              tolerations: []
              persistentVolume:
                enabled: true
              podLabels: {
                app: prometheus,
                version: "1"
              }
              podSecurityPolicy:
                annotations: {}
              replicaCount: 1
              statefulSet:
                enabled: false

                annotations: {}
                labels: {}
                podManagementPolicy: OrderedReady
                headless:
                  annotations: {}
                  labels: {}
                  enableMeshPeer: false

                  servicePort: 80
              resources: {}
                # limits:
                #   cpu: 10m
                #   memory: 32Mi
                # requests:
                #   cpu: 10m
                #   memory: 32Mi

              securityContext:
                runAsUser: 65534
                runAsNonRoot: true
                runAsGroup: 65534
                fsGroup: 65534

              ## Security context to be added to alertmanager container
              containerSecurityContext: {}

              service:
                annotations: {}
                labels: {}
                clusterIP: ""
                externalIPs: []

                loadBalancerIP: ""
                loadBalancerSourceRanges: []
                servicePort: 80
                # nodePort: 30000
                sessionAffinity: None
                type: ClusterIP
              clusterPeers: []
                resources: {}
              alertmanager:
                ## If false, the configmap-reload container will not be deployed
                ##
                enabled: true

                ## configmap-reload container name
                ##
                name: configmap-reload

                ## configmap-reload container image
                ##
                image:
                  repository: jimmidyson/configmap-reload
                  tag: v0.5.0
                  pullPolicy: IfNotPresent
            kubeStateMetrics:
              enabled: true
              podSecurityPolicy:
              priorityClassName: ""
              updateStrategy:
                type: RollingUpdate
              pod:
                labels: {
                  app: prometheus,
                  version: "1"
                }
              podDisruptionBudget:
                enabled: false
                maxUnavailable: 1
              resources: {}
                # limits:
                #   cpu: 200m
                #   memory: 50Mi
                # requests:
                #   cpu: 100m
                #   memory: 30Mi
              container:
                securityContext:
                  allowPrivilegeEscalation: false
              # Custom DNS configuration to be added to node-exporter pods
              service:
                annotations:
                  prometheus.io/scrape: "true"
                labels: {}
                clusterIP: ""
                externalIPs: []

                hostPort: 9100
                loadBalancerIP: ""
                loadBalancerSourceRanges: []
                servicePort: 9100
                type: ClusterIP

            server:
              ## Prometheus server container name
              ##
              enabled: true
              name: server
              sidecarContainers: {}
              sidecarTemplateValues: {}
              image:
                repository: quay.io/prometheus/prometheus
                tag: v2.36.2
                pullPolicy: IfNotPresent
              storagePath: ""

              global:
                ## How frequently to scrape targets by default
                ##
                scrape_interval: 1m
                ## How long until a scrape request times out
                ##
                scrape_timeout: 10s
                ## How frequently to evaluate rules
                ##
                evaluation_interval: 1m
              ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
              ##
              remoteWrite: []
              ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read
              ##
              remoteRead: []

              ## Custom HTTP headers for Liveness/Readiness/Startup Probe
              ##
              ## Useful for providing HTTP Basic Auth to healthchecks
              probeHeaders: []

              ## Additional Prometheus server container arguments
              ##
              extraArgs: {}

              ## Additional InitContainers to initialize the pod
              ##
              extraInitContainers: []

              ## Additional Prometheus server Volume mounts
              ##
              extraVolumeMounts: []

              ## Additional Prometheus server Volumes
              ##
              extraVolumes: []

              ## Additional Prometheus server hostPath mounts
              podLabels: {
                app: prometheus,
                version: "1"
              }

              podSecurityPolicy:
                annotations: {}
              replicaCount: 1

              resources: {}
                # limits:
                #   cpu: 500m
                #   memory: 512Mi
                # requests:
                #   cpu: 500m
                #   memory: 512Mi
              hostNetwork: false
              dnsPolicy: ClusterFirst
              verticalAutoscaler:
                ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)
                enabled: false
              dnsConfig: {}
                # nameservers:
                #   - 1.2.3.4
                # searches:
                #   - ns1.svc.cluster-domain.example
                #   - my.dns.search.suffix
                # options:
                #   - name: ndots
                #     value: "2"
              #   - name: edns0
              ## Security context to be added to server pods
              ##
              securityContext:
                runAsUser: 65534
                runAsNonRoot: true
                runAsGroup: 65534
                fsGroup: 65534

              ## Security context to be added to server container
              containerSecurityContext: {}

              service:
                ## If false, no Service will be created for the Prometheus server
                ##
                enabled: true

                annotations: {}
                labels: {}
                clusterIP: ""

                ## List of IP addresses at which the Prometheus server service is available
                ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
                ##
                externalIPs: []

                loadBalancerIP: ""
                loadBalancerSourceRanges: []
                servicePort: 80
                sessionAffinity: None
                type: ClusterIP

                ## Enable gRPC port on service to allow auto discovery with thanos-querier
                gRPC:
                  enabled: false
                  servicePort: 10901
                  # nodePort: 10901

                ## If using a statefulSet (statefulSet.enabled=true), configure the
                ## service to connect to a specific replica to have a consistent view
                ## of the data.
                statefulsetReplica:
                  enabled: false
                  replica: 0
            pushgateway:
              ## If false, pushgateway will not be installed
              ##
              enabled: true
              image:
                repository: prom/pushgateway
                tag: v1.4.3
                pullPolicy: IfNotPresent
              
              podLabels: {
                app: prometheus,
                version: "1"
              }
              replicaCount: 1
              verticalAutoscaler:
                ## If true a VPA object will be created for the controller
                enabled: false
                # updateMode: "Auto"
                # containerPolicies:
                # - containerName: 'prometheus-pushgateway'

              # Custom DNS configuration to be added to push-gateway pods
              dnsConfig: {}
                # nameservers:
                #   - 1.2.3.4
                # searches:
                #   - ns1.svc.cluster-domain.example
                #   - my.dns.search.suffix
                # options:
                #   - name: ndots
                #     value: "2"
              #   - name: edns0

                size: 2Gi
            alertmanagerFiles:
              alertmanager.yml:
                global: {}
                  # slack_api_url: ''

                receivers:
                  - name: default-receiver
                    # slack_configs:
                    #  - channel: '@you'
                    #    send_resolved: true

                route:
                  group_wait: 10s
                  group_interval: 5m
                  receiver: default-receiver
                  repeat_interval: 3h

            ## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)
            ruleFiles: {}

            ## Prometheus server ConfigMap entries
            ##
            serverFiles:

              ## Alerts configuration
              ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
              alerting_rules.yml: {}
              # groups:
              #   - name: Instances
              #     rules:
              #       - alert: InstanceDown
              #         expr: up == 0
              #         for: 5m
              #         labels:
              #           severity: page
              #         annotations:
              #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
              #           summary: 'Instance {{ $labels.instance }} down'
              ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml
              alerts: {}

              ## Records configuration
              ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/
              recording_rules.yml: {}
              ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml
              rules: {}

              prometheus.yml:
                rule_files:
                  - /etc/config/recording_rules.yml
                  - /etc/config/alerting_rules.yml
                ## Below two files are DEPRECATED will be removed from this default values file
                  - /etc/config/rules
                  - /etc/config/alerts

                scrape_configs:
                  - job_name: prometheus
                    static_configs:
                      - targets:

                  - job_name: 'istiod'
                    kubernetes_sd_configs:
                    - role: endpoints
                      namespaces:
                        names:
                        - istio-system
                    relabel_configs:
                    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                      action: keep
                      regex: istiod;http-monitoring

                  - job_name: 'envoy-stats'
                    metrics_path: /stats/prometheus
                    kubernetes_sd_configs:
                    - role: pod

                    relabel_configs:
                    - source_labels: [__meta_kubernetes_pod_container_port_name]
                      action: keep
                      regex: '.*-envoy-prom'


                  # - job_name: argocd-server-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-server-metrics.argocd.svc.cluster.local:8083
                  #   scheme: http

                  # - job_name: argocd-application-controller-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-application-controller-metrics.argocd.svc.cluster.local:8082
                  #   scheme: http


                  # - job_name: argocd-applicationset-controller-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-applicationset-controller-metrics.argocd.svc.cluster.local:8085
                  #   scheme: http

                  # - job_name: argocd-redis-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-redis-metrics.argocd.svc.cluster.local:9121
                  #   scheme: http

                  # - job_name: argocd-dex-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-dex-server.argocd.svc.cluster.local:5558
                  #   scheme: http


                  # - job_name: argocd-notifications-metrics
                  #   static_configs:
                  #     - targets:
                  #       - argocd-notifications-controller-metrics.argocd.svc.cluster.local:9001
                  #   scheme: http

                  # A scrape configuration for running Prometheus on a Kubernetes cluster.
                  # This uses separate scrape configs for cluster components (i.e. API server, node)
                  # and services to allow each to use different authentication configs.
                  #
                  # Kubernetes labels will be added as Prometheus labels on metrics via the
                  # `labelmap` relabeling action.

                  # Scrape config for API servers.
                  #
                  # Kubernetes exposes API servers as endpoints to the default/kubernetes
                  # service so this uses `endpoints` role and uses relabelling to only keep
                  # the endpoints associated with the default/kubernetes service using the
                  # default named port `https`. This works for single API server deployments as
                  # well as HA API server deployments.
                  - job_name: 'kubernetes-apiservers'

                    kubernetes_sd_configs:
                      - role: endpoints

                    # Default to scraping over https. If required, just disable this or change to
                    # `http`.
                    scheme: https

                    # This TLS & bearer token file config is used to connect to the actual scrape
                    # endpoints for cluster components. This is separate to discovery auth
                    # configuration because discovery & scraping are two separate concerns in
                    # Prometheus. The discovery auth config is automatic if Prometheus runs inside
                    # the cluster. Otherwise, more config options have to be provided within the
                    # <kubernetes_sd_config>.
                    tls_config:
                      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                      # If your node certificates are self-signed or use a different CA to the
                      # master CA, then disable certificate verification below. Note that
                      # certificate verification is an integral part of a secure infrastructure
                      # so this should only be disabled in a controlled environment. You can
                      # disable certificate verification by uncommenting the line below.
                      #
                      insecure_skip_verify: true
                    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

                    # Keep only the default/kubernetes service endpoints for the https port. This
                    # will add targets for each API server which Kubernetes adds an endpoint to
                    # the default/kubernetes service.
                    relabel_configs:
                      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                        action: keep
                        regex: default;kubernetes;https

                  - job_name: 'kubernetes-nodes'

                    # Default to scraping over https. If required, just disable this or change to
                    # `http`.
                    scheme: https

                    # This TLS & bearer token file config is used to connect to the actual scrape
                    # endpoints for cluster components. This is separate to discovery auth
                    # configuration because discovery & scraping are two separate concerns in
                    # Prometheus. The discovery auth config is automatic if Prometheus runs inside
                    # the cluster. Otherwise, more config options have to be provided within the
                    # <kubernetes_sd_config>.
                    tls_config:
                      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                      # If your node certificates are self-signed or use a different CA to the
                      # master CA, then disable certificate verification below. Note that
                      # certificate verification is an integral part of a secure infrastructure
                      # so this should only be disabled in a controlled environment. You can
                      # disable certificate verification by uncommenting the line below.
                      #
                      insecure_skip_verify: true
                    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

                    kubernetes_sd_configs:
                      - role: node

                    relabel_configs:
                      - action: labelmap
                        regex: __meta_kubernetes_node_label_(.+)
                      - target_label: __address__
                        replacement: kubernetes.default.svc:443
                      - source_labels: [__meta_kubernetes_node_name]
                        regex: (.+)
                        target_label: __metrics_path__
                        replacement: /api/v1/nodes/$1/proxy/metrics


                  - job_name: 'kubernetes-nodes-cadvisor'

                    # Default to scraping over https. If required, just disable this or change to
                    # `http`.
                    scheme: https

                    # This TLS & bearer token file config is used to connect to the actual scrape
                    # endpoints for cluster components. This is separate to discovery auth
                    # configuration because discovery & scraping are two separate concerns in
                    # Prometheus. The discovery auth config is automatic if Prometheus runs inside
                    # the cluster. Otherwise, more config options have to be provided within the
                    # <kubernetes_sd_config>.
                    tls_config:
                      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                      # If your node certificates are self-signed or use a different CA to the
                      # master CA, then disable certificate verification below. Note that
                      # certificate verification is an integral part of a secure infrastructure
                      # so this should only be disabled in a controlled environment. You can
                      # disable certificate verification by uncommenting the line below.
                      #
                      insecure_skip_verify: true
                    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

                    kubernetes_sd_configs:
                      - role: node

                    # This configuration will work only on kubelet 1.7.3+
                    # As the scrape endpoints for cAdvisor have changed
                    # if you are using older version you need to change the replacement to
                    # replacement: /api/v1/nodes/$1:4194/proxy/metrics
                    # more info here https://github.com/coreos/prometheus-operator/issues/633
                    relabel_configs:
                      - action: labelmap
                        regex: __meta_kubernetes_node_label_(.+)
                      - target_label: __address__
                        replacement: kubernetes.default.svc:443
                      - source_labels: [__meta_kubernetes_node_name]
                        regex: (.+)
                        target_label: __metrics_path__
                        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor

                  # Scrape config for service endpoints.
                  #
                  # The relabeling allows the actual service scrape endpoint to be configured
                  # via the following annotations:
                  #
                  # * `prometheus.io/scrape`: Only scrape services that have a value of
                  # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
                  # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
                  # to set this to `https` & most likely set the `tls_config` of the scrape config.
                  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
                  # * `prometheus.io/port`: If the metrics are exposed on a different port to the
                  # service then set this appropriately.
                  # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
                  # then you can set any parameter
                  - job_name: 'kubernetes-service-endpoints'
                    honor_labels: true

                    kubernetes_sd_configs:
                      - role: endpoints

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                        action: keep
                        regex: true
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
                        action: drop
                        regex: true
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                        action: replace
                        target_label: __scheme__
                        regex: (https?)
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                        action: replace
                        target_label: __metrics_path__
                        regex: (.+)
                      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                        action: replace
                        target_label: __address__
                        regex: (.+?)(?::\d+)?;(\d+)
                        replacement: $1:$2
                      - action: labelmap
                        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
                        replacement: __param_$1
                      - action: labelmap
                        regex: __meta_kubernetes_service_label_(.+)
                      - source_labels: [__meta_kubernetes_namespace]
                        action: replace
                        target_label: namespace
                      - source_labels: [__meta_kubernetes_service_name]
                        action: replace
                        target_label: service
                      - source_labels: [__meta_kubernetes_pod_node_name]
                        action: replace
                        target_label: node

                  # Scrape config for slow service endpoints; same as above, but with a larger
                  # timeout and a larger interval
                  #
                  # The relabeling allows the actual service scrape endpoint to be configured
                  # via the following annotations:
                  #
                  # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
                  # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
                  # to set this to `https` & most likely set the `tls_config` of the scrape config.
                  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
                  # * `prometheus.io/port`: If the metrics are exposed on a different port to the
                  # service then set this appropriately.
                  # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
                  # then you can set any parameter
                  - job_name: 'kubernetes-service-endpoints-slow'
                    honor_labels: true

                    scrape_interval: 5m
                    scrape_timeout: 30s

                    kubernetes_sd_configs:
                      - role: endpoints

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]
                        action: keep
                        regex: true
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                        action: replace
                        target_label: __scheme__
                        regex: (https?)
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                        action: replace
                        target_label: __metrics_path__
                        regex: (.+)
                      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                        action: replace
                        target_label: __address__
                        regex: (.+?)(?::\d+)?;(\d+)
                        replacement: $1:$2
                      - action: labelmap
                        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
                        replacement: __param_$1
                      - action: labelmap
                        regex: __meta_kubernetes_service_label_(.+)
                      - source_labels: [__meta_kubernetes_namespace]
                        action: replace
                        target_label: namespace
                      - source_labels: [__meta_kubernetes_service_name]
                        action: replace
                        target_label: service
                      - source_labels: [__meta_kubernetes_pod_node_name]
                        action: replace
                        target_label: node

                  - job_name: 'prometheus-pushgateway'
                    honor_labels: true

                    kubernetes_sd_configs:
                      - role: service

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                        action: keep
                        regex: pushgateway

                  # Example scrape config for probing services via the Blackbox Exporter.
                  #
                  # The relabeling allows the actual service scrape endpoint to be configured
                  # via the following annotations:
                  #
                  # * `prometheus.io/probe`: Only probe services that have a value of `true`
                  - job_name: 'kubernetes-services'
                    honor_labels: true

                    metrics_path: /probe
                    params:
                      module: [http_2xx]

                    kubernetes_sd_configs:
                      - role: service

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                        action: keep
                        regex: true
                      - source_labels: [__address__]
                        target_label: __param_target
                      - target_label: __address__
                        replacement: blackbox
                      - source_labels: [__param_target]
                        target_label: instance
                      - action: labelmap
                        regex: __meta_kubernetes_service_label_(.+)
                      - source_labels: [__meta_kubernetes_namespace]
                        target_label: namespace
                      - source_labels: [__meta_kubernetes_service_name]
                        target_label: service

                  # Example scrape config for pods
                  #
                  # The relabeling allows the actual pod scrape endpoint to be configured via the
                  # following annotations:
                  #
                  # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,
                  # except if `prometheus.io/scrape-slow` is set to `true` as well.
                  # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
                  # to set this to `https` & most likely set the `tls_config` of the scrape config.
                  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
                  # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
                  - job_name: 'kubernetes-pods'
                    honor_labels: true

                    kubernetes_sd_configs:
                      - role: pod

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                        action: keep
                        regex: true
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
                        action: drop
                        regex: true
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                        action: replace
                        regex: (https?)
                        target_label: __scheme__
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                        action: replace
                        target_label: __metrics_path__
                        regex: (.+)
                      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                        action: replace
                        regex: (.+?)(?::\d+)?;(\d+)
                        replacement: $1:$2
                        target_label: __address__
                      - action: labelmap
                        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
                        replacement: __param_$1
                      - action: labelmap
                        regex: __meta_kubernetes_pod_label_(.+)
                      - source_labels: [__meta_kubernetes_namespace]
                        action: replace
                        target_label: namespace
                      - source_labels: [__meta_kubernetes_pod_name]
                        action: replace
                        target_label: pod
                      - source_labels: [__meta_kubernetes_pod_phase]
                        regex: Pending|Succeeded|Failed|Completed
                        action: drop

                  # Example Scrape config for pods which should be scraped slower. An useful example
                  # would be stackriver-exporter which queries an API on every scrape of the pod
                  #
                  # The relabeling allows the actual pod scrape endpoint to be configured via the
                  # following annotations:
                  #
                  # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`
                  # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
                  # to set this to `https` & most likely set the `tls_config` of the scrape config.
                  # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
                  # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
                  - job_name: 'kubernetes-pods-slow'
                    honor_labels: true

                    scrape_interval: 5m
                    scrape_timeout: 30s

                    kubernetes_sd_configs:
                      - role: pod

                    relabel_configs:
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]
                        action: keep
                        regex: true
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                        action: replace
                        regex: (https?)
                        target_label: __scheme__
                      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                        action: replace
                        target_label: __metrics_path__
                        regex: (.+)
                      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                        action: replace
                        regex: (.+?)(?::\d+)?;(\d+)
                        replacement: $1:$2
                        target_label: __address__
                      - action: labelmap
                        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
                        replacement: __param_$1
                      - action: labelmap
                        regex: __meta_kubernetes_pod_label_(.+)
                      - source_labels: [__meta_kubernetes_namespace]
                        action: replace
                        target_label: namespace
                      - source_labels: [__meta_kubernetes_pod_name]
                        action: replace
                        target_label: pod
                      - source_labels: [__meta_kubernetes_pod_phase]
                        regex: Pending|Succeeded|Failed|Completed
                        action: drop

            # adds additional scrape configs to prometheus.yml
            # must be a string so you have to add a | after extraScrapeConfigs:
            # example adds prometheus-blackbox-exporter scrape config
            extraScrapeConfigs:
              # - job_name: 'prometheus-blackbox-exporter'
              #   metrics_path: /probe
              #   params:
              #     module: [http_2xx]
              #   static_configs:
              #     - targets:
              #       - https://example.com
              #   relabel_configs:
              #     - source_labels: [__address__]
              #       target_label: __param_target
              #     - source_labels: [__param_target]
              #       target_label: instance
              #     - target_label: __address__
              #       replacement: prometheus-blackbox-exporter:9115

            # Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager
            # useful in H/A prometheus with different external labels but the same alerts
            alertRelabelConfigs:
              # alert_relabel_configs:
              # - source_labels: [dc]
              #   regex: (.+)\d+
              #   target_label: dc

            networkPolicy:
              ## Enable creation of NetworkPolicy resources.
              ##
              enabled: false

            # Force namespace of namespaced resources
            forceNamespace: null

            # Extra manifests to deploy as an array
            extraManifests: []
              # - apiVersion: v1
              #   kind: ConfigMap
              #   metadata:
              #   labels:
              #     name: prometheus-extra
              #   data:
              #     extra-data: "value"



      destination:
        server: '{{url}}'
        namespace: prometheus

      syncPolicy:
        automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.
          prune: false # Specifies if resources should be pruned during auto-syncing ( false by default ).
          selfHeal: false # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).
          allowEmpty: false # Allows deleting all application resources during automatic syncing ( false by default ).
        syncOptions:     # Sync options which modifies sync behavior
        - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).
        - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.
        - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.
        - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation
        # The retry feature is available since v1.7
        retry:
          limit: 5 # number of failed sync attempt retries; unlimited number of attempts if less than 0
          backoff:
            duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m", "1h")
            factor: 2 # a factor to multiply the base duration after each failed retry
            maxDuration: 3m # the maximum amount of time allowed for the backoff strategy
